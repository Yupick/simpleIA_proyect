# simpleIA_proyect ‚Äî LLM local primero (con soporte para nube)

Este proyecto implementa un sistema LLM modular con FastAPI y Hugging Face Transformers, dise√±ado con la filosof√≠a local-first: su funci√≥n principal es correr modelos en tu propia m√°quina, con la opci√≥n de integrar modelos hospedados en la nube mediante una capa de proveedores.

## Caracter√≠sticas M1 (Completado)

- ‚úÖ **Local primero**: ejecuta e infiere con modelos locales (CPU o GPU si disponible)
- ‚úÖ **API modular**: endpoints separados por dominio (auth, predict, model, feedback)
- ‚úÖ **Autenticaci√≥n JWT** y almacenamiento en SQLite
- ‚úÖ **Feedback** para reentrenamiento
- ‚úÖ **Rate limiting** simple para `/predict` configurable por entorno
- ‚úÖ **Sanitizaci√≥n XSS** en feedback
- ‚úÖ **Tests b√°sicos**: autenticaci√≥n, predicci√≥n, feedback, m√©tricas

## Caracter√≠sticas M2 (Completado) üöÄ

### Seguridad Mejorada

- ‚úÖ **Cookies Secure** condicionales seg√∫n `ENVIRONMENT` (production/development)
- ‚úÖ **datetime.now(timezone.utc)** en lugar de utcnow deprecated
- ‚úÖ **Sanitizaci√≥n XSS avanzada** con html.escape y regex

### Configuraci√≥n Modernizada

- ‚úÖ **Pydantic Settings V2** con SettingsConfigDict
- ‚úÖ **Lifespan events** (@asynccontextmanager) reemplaza @on_event deprecated
- ‚úÖ **Selecci√≥n dispositivo** (CPU/CUDA) via settings.DEVICE

### Providers Multi-LLM

- ‚úÖ **ClaudeProvider**: integraci√≥n con Anthropic API usando httpx
- ‚úÖ **OpenAIProvider**: integraci√≥n con OpenAI API
- ‚úÖ **HuggingFaceProvider**: local transformers
- ‚úÖ **Switching din√°mico** via config.provider (hf/claude/openai)

### Performance y Cach√©

- ‚úÖ **Cache LRU** para respuestas LLM con TTL y hash SHA256
- ‚úÖ **Streaming SSE**: StreamingResponse para tokens en tiempo real
- ‚úÖ **Cache hit/miss tracking** con estad√≠sticas

### Embeddings y B√∫squeda Sem√°ntica

- ‚úÖ **sentence-transformers**: modelo all-MiniLM-L6-v2 (384 dims)
- ‚úÖ **FAISS vector store**: b√∫squeda L2 similarity
- ‚úÖ **Endpoints /embed**: encode, add, search, save, load, stats
- ‚úÖ **Persistencia**: save/load √≠ndice FAISS + documentos pickle

### Dashboard y M√©tricas

- ‚úÖ **Dashboard admin**: Chart.js con gr√°ficos de requests, latency, status, feedback
- ‚úÖ **M√©tricas training**: SQLite para loss por epoch
- ‚úÖ **Endpoints /training**: runs, metrics, latest
- ‚úÖ **Auto-refresh** dashboard cada 30s

### Infraestructura

- ‚úÖ **Docker completo**: Dockerfile multi-stage + docker-compose.yml
- ‚úÖ **.dockerignore** optimizado
- ‚úÖ **run_llm.sh mejorado**: comandos all/trainer/api/client/line/admin
- ‚úÖ **admin_cli.py**: herramientas CLI (feedback list, model reload)
- ‚úÖ **.gitignore completo**: modelos, embeddings, caches, notebooks

### Tests M2 (Creados)

- ‚úÖ **test_providers.py**: ClaudeProvider, OpenAIProvider, initialization
- ‚úÖ **test_cache.py**: LRU eviction, TTL, stats, hash collision
- ‚úÖ **test_streaming.py**: SSE events, cache integration, error handling
- ‚úÖ **test_embeddings.py**: encode, search, FAISS, endpoints
- ‚ö†Ô∏è **Nota**: Tests M2 requieren ajustes para coincidir con implementaci√≥n real; tests M1 (7) pasan correctamente

Estructura del proyecto (resumen)

- `config/`: configuraci√≥n (`config.json`)
- `app/`: c√≥digo de la aplicaci√≥n (API modular, seguridad, modelo, DB, providers, training)
  - `app/main.py`: entrypoint de la API modular con lifespan events
  - `app/api/routers/`: rutas `auth`, `predict`, `model`, `feedback`, `embeddings`, `admin`, `training`
  - `app/models/`: gestor de modelo (`model_manager.py`), embeddings (`embeddings.py`)
  - `app/security/`: JWT y hashing con get_current_user
  - `app/db/`: SQLite helpers (`sqlite.py`, `training_metrics.py`)
  - `app/providers/`: ClaudeProvider, OpenAIProvider, HuggingFaceProvider
  - `app/training/`: trainer unificado
  - `app/core/`: cache LRU, settings Pydantic V2, config, logging, metrics, rate_limit
- `model_llm/`: checkpoints y modelos entrenados/locales
- `templates/`: HTML para cliente web (index.html con streaming) y dashboard admin
- `feedback/`: bases SQLite de usuarios, feedback y m√©tricas training
- `data/embeddings/`: √≠ndices FAISS y documentos
- `requirements.txt`: dependencias (numpy<2.0.0, sentence-transformers, faiss-cpu)
- `run_llm.sh`: script control servicios (all/trainer/api/client/line/admin)
- `Dockerfile` + `docker-compose.yml`: despliegue containerizado

Requisitos

- Python 3.10+ recomendado.
- pip reciente (`pip>=23`).
- Opcional: CUDA/cuDNN para acelerar inferencia/entrenamiento con PyTorch.

Instalaci√≥n r√°pida

```bash
./setup_env.sh
# o manualmente
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
```

Configuraci√≥n (.env y config.json)

1. Copia el ejemplo y ed√≠talo:

```bash
cp .env.example .env
```

Variables importantes:

- `SECRET_KEY`: cambia por un valor seguro (generado con `secrets.token_urlsafe(32)`)
- `ENVIRONMENT`: `development` o `production` (activa cookies Secure)
- `DEFAULT_MODEL`: nombre o ruta local del modelo (p. ej., `gpt2` o `model_llm/mi_modelo_local`)
- `DEVICE`: `cpu` o `cuda` (selecci√≥n autom√°tica GPU si disponible)
- `ANTHROPIC_API_KEY`: API key para ClaudeProvider (opcional)
- `OPENAI_API_KEY`: API key para OpenAIProvider (opcional)
- `NUM_TRAIN_EPOCHS`, `TRAIN_BATCH_SIZE`: par√°metros de entrenamiento por defecto

2. `config/config.json` controla principalmente el modelo seleccionado en caliente:

```json
{
  "selected_model": "flax-community/gpt-2-spanish",
  "provider": "hf"
}
```

Providers disponibles:

- `hf`: HuggingFace local (transformers)
- `claude`: Anthropic Claude API (requiere ANTHROPIC_API_KEY)
- `openai`: OpenAI API (requiere OPENAI_API_KEY)

Proveedor LLM

- Definir en `.env` o `config.json` la clave `LLM_PROVIDER` o `provider` (`hf` por defecto).
- Valores previstos futuros: `hf`, `claude`, `openai`, `custom`.
- Si no es `hf`, se usa por ahora el wrapper HuggingFace como placeholder (internamente igual, pero deja preparada la rama l√≥gica).

Rate Limiting

- Variables de entorno:
  - `RATE_LIMIT_REQUESTS` (default 10)
  - `RATE_LIMIT_WINDOW_SECONDS` (default 60)
- Un bucket por IP cliente. Respuesta `429` si se excede.

Ejecutar (API modular local)

```bash
# Opci√≥n 1: Script run_llm.sh (recomendado)
./run_llm.sh all          # API + Cliente Web
./run_llm.sh api          # Solo API en :8000
./run_llm.sh client       # Solo Cliente Web en :8001
./run_llm.sh trainer      # Entrenador
./run_llm.sh line         # Cliente CLI
./run_llm.sh admin feedback  # Listar feedback
./run_llm.sh admin reload    # Recargar modelo

# Opci√≥n 2: Uvicorn directo
source venv/bin/activate
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Salud del servicio: `GET http://localhost:8000/health`

Endpoints principales

### Autenticaci√≥n

- `POST /auth/register`: registrar usuario. Body JSON `{ "username", "password" }`
- `POST /auth/login`: login OAuth2 Password ‚Üí `{ access_token, token_type }`

### Modelo

- `GET /model`: modelo actual y provider
- `POST /model`: cambiar modelo `{ "model_name": "gpt2" }` (acepta ruta local o id HF). Recarga el modelo

### Predicci√≥n

- `POST /predict`: inferencia `{ "prompt", "max_length", "num_return_sequences", "temperature", "stream": bool }`
  - `stream=false`: respuesta JSON completa
  - `stream=true`: StreamingResponse SSE (text/event-stream)

### Embeddings (M2)

- `POST /embed/encode`: generar embeddings `{ "texts": ["text1", "text2"] }`
- `POST /embed/add`: agregar documentos al √≠ndice `{ "documents": ["doc1", "doc2"] }`
- `POST /embed/search`: buscar similares `{ "query": "text", "k": 5 }`
- `POST /embed/save`: guardar √≠ndice FAISS
- `POST /embed/load`: cargar √≠ndice guardado
- `GET /embed/stats`: estad√≠sticas del √≠ndice

### Dashboard y M√©tricas (M2)

- `GET /admin`: dashboard Chart.js (requiere autenticaci√≥n)
- `GET /metrics`: m√©tricas Prometheus-style
- `GET /training/runs`: listar runs de entrenamiento
- `GET /training/runs/{id}/metrics`: m√©tricas de un run
- `GET /training/latest`: √∫ltimo run de entrenamiento

### Feedback

- `POST /feedback`: almacenar feedback `{ "text" }` (l√≠mite 5000 chars, sanitizaci√≥n XSS)

Ejemplos r√°pidos (curl)

```bash
# Registro
curl -X POST http://localhost:8000/auth/register \
	-H 'Content-Type: application/json' \
	-d '{"username":"demo","password":"demo123"}'

# Login
TOKEN=$(curl -s -X POST http://localhost:8000/auth/login \
	-H 'Content-Type: application/x-www-form-urlencoded' \
	-d 'username=demo&password=demo123' | jq -r .access_token)

# Predicci√≥n (opcionalmente con token)
curl -X POST http://localhost:8000/predict \
	-H 'Content-Type: application/json' \
	-H "Authorization: Bearer $TOKEN" \
	-d '{"prompt":"Hola, ¬øqui√©n eres?","max_length":50}'

# Cambiar modelo (HF o carpeta local)
curl -X POST http://localhost:8000/model \
	-H 'Content-Type: application/json' \
	-d '{"model_name":"gpt2"}'
```

Local vs. nube (providers)

- **HuggingFace (hf)**: Modelos locales con `transformers.from_pretrained`. Si apuntas a carpeta en `model_llm/`, se carga desde disco.
- **Claude (claude)**: Integraci√≥n con Anthropic API via httpx. Requiere `ANTHROPIC_API_KEY` en `.env`.
- **OpenAI (openai)**: Integraci√≥n con OpenAI API via httpx. Requiere `OPENAI_API_KEY` en `.env`.

Cambiar provider en runtime:

```bash
# Via config.json
echo '{"selected_model":"gpt2","provider":"hf"}' > config/config.json

# O via admin CLI
./run_llm.sh admin reload
```

La arquitectura `app/providers/` permite a√±adir proveedores externos sin cambiar l√≥gica de negocio.

Entrenamiento local (b√°sico)

- Opci√≥n 1 (legacy): usar `app/llm_trainer.py` existentes para flujos de fine-tuning con ficheros en `trainer_llm/`.
- Opci√≥n 2 (unificado en progreso): `app/training/trainer.py` expone una funci√≥n `train(model_name, lines)` para integrar un pipeline m√°s limpio. Los checkpoints se guardan en `model_llm/`; luego puedes seleccionarlos con `POST /model` indicando la ruta local.

Cliente web y CLI

- **Cliente web**: `http://localhost:8001` - Interfaz HTML con streaming SSE, autenticaci√≥n JWT via cookies
- **Cliente CLI**: `./run_llm.sh line` - Prompt interactivo contra `/predict`
- **Admin CLI**: `./run_llm.sh admin feedback|reload` - Herramientas administraci√≥n

## Cach√© y Performance (M2)

### Cache LRU

- **Hash SHA256** de prompt+params como key
- **TTL configurable** (default 3600s)
- **Eviction LRU** al alcanzar max_size
- **Estad√≠sticas**: hits, misses, hit_rate via `/metrics`

### Streaming SSE

- **Server-Sent Events** para tokens en tiempo real
- **Formato**: `data: <token>\n\n`
- **Cliente JS** con EventSource simulado
- **Cache bypass** autom√°tico en streaming

## Embeddings y B√∫squeda (M2)

### Modelo

- **sentence-transformers/all-MiniLM-L6-v2**
- **384 dimensiones**
- **Normalizaci√≥n L2** autom√°tica

### FAISS Vector Store

- **IndexFlatL2** para b√∫squeda exhaustiva
- **Persistencia** a disco (.faiss + .pkl)
- **Add documents** con embeddings batch
- **Search** por similitud coseno/L2

### Casos de uso

- B√∫squeda sem√°ntica en documentaci√≥n
- RAG (Retrieval Augmented Generation)
- Similar questions matching
- Knowledge base search

Seguridad

- ‚úÖ **SECRET_KEY** en `.env` - JWT signing
- ‚úÖ **HTTPS** en producci√≥n - cookies seguras condicionales
- ‚úÖ **Sanitizaci√≥n XSS** - html.escape + regex en feedback
- ‚úÖ **Rate limiting** - configurable por entorno (default 10 req/60s)
- ‚úÖ **Autenticaci√≥n JWT** - tokens con expiraci√≥n
- ‚úÖ **CORS configurado** - origins permitidos via settings
- ‚ö†Ô∏è **Tokens en cookies** - SameSite=Lax, Secure en production
- ‚ö†Ô∏è **Validaci√≥n inputs** - Pydantic models en todos los endpoints

Recomendaciones producci√≥n:

1. Generar SECRET_KEY: `python -c "import secrets; print(secrets.token_urlsafe(32))"`
2. Set `ENVIRONMENT=production` en `.env`
3. Usar reverse proxy (nginx/traefik) con HTTPS
4. Rate limiting por IP en load balancer
5. Logs centralizados y monitoring

Notas de migraci√≥n

- Se ha modularizado la API; `run_llm.sh` a√∫n referencia el entrypoint legacy. Recomendado invocar `uvicorn app.main:app` directamente.
- Los trainers legacy seguir√°n disponibles mientras se completa la migraci√≥n al trainer unificado.

## Despliegue con Docker

### Construcci√≥n y ejecuci√≥n

```bash
# Construcci√≥n de imagen
docker build -t llm-modular-api .

# Ejecuci√≥n simple
docker run -p 8000:8000 \
  -e SECRET_KEY="tu_clave_segura" \
  -v $(pwd)/feedback:/app/feedback \
  -v $(pwd)/model_llm:/app/model_llm \
  llm-modular-api

# Con docker-compose (recomendado)
docker-compose up -d
```

### Variables de entorno importantes

- `SECRET_KEY`: Clave para JWT (obligatoria en producci√≥n).
- `ENVIRONMENT`: `development` o `production` (activa cookies Secure).
- `DEVICE`: `cpu` o `cuda` (si GPU disponible).
- `DEFAULT_MODEL`: Modelo por defecto (puede ser ID HuggingFace o ruta local).
- `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`: Para proveedores externos (opcional).

### Vol√∫menes persistentes

- `./feedback`: Bases de datos SQLite (usuarios, feedback).
- `./model_llm`: Modelos locales entrenados/personalizados.
- `huggingface_cache`: Cache de modelos descargados de HuggingFace.

### Acceso a servicios

- API: `http://localhost:8000`
- Cliente web: `http://localhost:8001` (si se levanta el servicio `llm-client`)
- Health check: `http://localhost:8000/health`
- M√©tricas: `http://localhost:8000/metrics`

### Producci√≥n

1. Generar SECRET_KEY seguro:

```bash
python -c "import secrets; print(secrets.token_urlsafe(32))"
```

2. Configurar `.env` con valores de producci√≥n.

3. Usar reverse proxy (nginx/traefik) con HTTPS.

4. Ajustar l√≠mites de recursos en `docker-compose.yml`:

```yaml
deploy:
  resources:
    limits:
      cpus: "2"
      memory: 4G
```

### Troubleshooting Docker

- **Error de permisos en vol√∫menes**: Asegurar que los directorios `feedback/` y `model_llm/` tengan permisos de escritura.
- **Modelo no carga**: Verificar que la variable `DEFAULT_MODEL` apunte a un modelo v√°lido o que est√© en cache.
- **GPU no detectada**: Instalar `nvidia-docker` y usar imagen base con CUDA.

Soluci√≥n de problemas

### Memoria insuficiente

- Cambiar `DEFAULT_MODEL`/`selected_model` por modelo m√°s peque√±o (ej. `gpt2`)
- Usar `DEVICE=cpu` si GPU no disponible
- Reducir `TRAIN_BATCH_SIZE` en entrenamiento

### Dependencias

```bash
pip install -r requirements.txt
# Si falla NumPy/torch:
pip install "numpy<2.0.0" torch==2.2.1 --force-reinstall
```

### Provider switching no funciona

- Verificar `config/config.json` tiene `provider` correcto
- Reload modelo: `./run_llm.sh admin reload`
- Check logs: API key presente para claude/openai

### Embeddings lentos

- Primera ejecuci√≥n descarga modelo (3GB)
- Cache en `~/.cache/torch/sentence_transformers/`
- Usar `DEVICE=cuda` si GPU disponible

### Tests fallan

```bash
# Tests M1 (deben pasar):
venv/bin/python -m pytest tests/test_auth.py tests/test_predict.py tests/test_feedback.py -v

# Tests M2 (requieren ajustes):
# Actualmente creados pero necesitan alinearse con implementaci√≥n real
```

### Docker

- **Permisos vol√∫menes**: `chmod 777 feedback/ model_llm/`
- **GPU no detectada**: instalar nvidia-docker, usar imagen CUDA
- **Modelo no carga**: verificar DEFAULT_MODEL v√°lido o en cache

## Pr√≥ximos Pasos (M3 - Futuro)

- [ ] **Tests M2 completos**: ajustar test_providers.py, test_cache.py, test_streaming.py, test_embeddings.py
- [ ] **RAG Pipeline**: integraci√≥n embeddings + retrieval + generation
- [ ] **Fine-tuning UI**: dashboard para entrenar modelos custom
- [ ] **Multi-model inference**: servir m√∫ltiples modelos simult√°neamente
- [ ] **Async streaming**: mejorar performance con async generators
- [ ] **Prometheus metrics**: exportar m√©tricas detalladas
- [ ] **WebSocket support**: alternativa a SSE para streaming
- [ ] **Model quantization**: GGUF, AWQ para modelos m√°s ligeros
- [ ] **Kubernetes deployment**: Helm charts y manifests
- [ ] **CI/CD pipeline**: GitHub Actions para tests y deployment

## Licencia

No especificada. A√±ade un archivo `LICENSE` si corresponde.

## Contribuir

1. Fork el repositorio
2. Crear branch feature: `git checkout -b feature/amazing-feature`
3. Commit cambios: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Abrir Pull Request

## Changelog

### M2 (Noviembre 2025)

- ‚úÖ Providers Claude/OpenAI integrados
- ‚úÖ Cache LRU con TTL
- ‚úÖ Streaming SSE
- ‚úÖ Embeddings FAISS + sentence-transformers
- ‚úÖ Dashboard admin Chart.js
- ‚úÖ Docker production-ready
- ‚úÖ Pydantic Settings V2
- ‚úÖ Security hardening (XSS, cookies secure)

### M1 (Inicial)

- ‚úÖ API modular FastAPI
- ‚úÖ Autenticaci√≥n JWT
- ‚úÖ Rate limiting
- ‚úÖ Feedback storage
- ‚úÖ HuggingFace integration
- ‚úÖ Tests b√°sicos
